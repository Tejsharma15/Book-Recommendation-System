{"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.10.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# **Install,Seting up selenium(For kaggle)**","metadata":{}},{"cell_type":"code","source":"# install google chrome\n!wget https://dl.google.com/linux/linux_signing_key.pub\n!sudo apt-key add linux_signing_key.pub\n!echo 'deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main' >> /etc/apt/sources.list.d/google-chrome.list\n!sudo apt-get -y update\n!sudo apt-get install -y google-chrome-stable","metadata":{"execution":{"iopub.status.busy":"2023-05-07T17:18:03.196567Z","iopub.execute_input":"2023-05-07T17:18:03.196920Z","iopub.status.idle":"2023-05-07T17:18:36.848823Z","shell.execute_reply.started":"2023-05-07T17:18:03.196892Z","shell.execute_reply":"2023-05-07T17:18:36.847638Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"--2023-05-07 17:18:04--  https://dl.google.com/linux/linux_signing_key.pub\nResolving dl.google.com (dl.google.com)... 74.125.204.190, 74.125.204.91, 74.125.204.93, ...\nConnecting to dl.google.com (dl.google.com)|74.125.204.190|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 14605 (14K) [application/octet-stream]\nSaving to: ‘linux_signing_key.pub’\n\nlinux_signing_key.p 100%[===================>]  14.26K  --.-KB/s    in 0s      \n\n2023-05-07 17:18:04 (66.9 MB/s) - ‘linux_signing_key.pub’ saved [14605/14605]\n\nOK\nGet:1 http://dl.google.com/linux/chrome/deb stable InRelease [1825 B]\nGet:2 http://packages.cloud.google.com/apt gcsfuse-focal InRelease [5002 B]    \nGet:3 https://packages.cloud.google.com/apt cloud-sdk InRelease [6361 B]       \nGet:4 http://dl.google.com/linux/chrome/deb stable/main amd64 Packages [1077 B]\nGet:5 http://packages.cloud.google.com/apt gcsfuse-focal/main amd64 Packages [2217 B]\nGet:6 https://packages.cloud.google.com/apt cloud-sdk/main amd64 Packages [434 kB]\nHit:7 http://archive.ubuntu.com/ubuntu focal InRelease                   \nGet:8 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\nGet:9 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]        \nGet:10 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [2203 kB]\nGet:11 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]\nGet:12 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [2341 kB]\nGet:13 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [2669 kB]\nGet:14 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [31.2 kB]\nGet:15 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1334 kB]\nGet:16 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1039 kB]\nGet:17 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [3150 kB]\nFetched 13.6 MB in 4s (3637 kB/s)                            \nReading package lists... Done\nReading package lists... Done\nBuilding dependency tree       \nReading state information... Done\nThe following additional packages will be installed:\n  fonts-liberation libgbm1 libu2f-udev libudev1 libwayland-server0 udev\nThe following NEW packages will be installed:\n  fonts-liberation google-chrome-stable libgbm1 libu2f-udev libwayland-server0\n  udev\nThe following packages will be upgraded:\n  libudev1\n1 upgraded, 6 newly installed, 0 to remove and 20 not upgraded.\nNeed to get 97.3 MB of archives.\nAfter this operation, 326 MB of additional disk space will be used.\nGet:1 http://dl.google.com/linux/chrome/deb stable/main amd64 google-chrome-stable amd64 113.0.5672.63-1 [95.0 MB]\nGet:2 http://archive.ubuntu.com/ubuntu focal/main amd64 fonts-liberation all 1:1.07.4-11 [822 kB]\nGet:3 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libwayland-server0 amd64 1.18.0-1ubuntu0.1 [31.3 kB]\nGet:4 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libgbm1 amd64 21.2.6-0ubuntu0.1~20.04.2 [29.2 kB]\nGet:5 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 libudev1 amd64 245.4-4ubuntu3.21 [75.9 kB]\nGet:6 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 udev amd64 245.4-4ubuntu3.21 [1366 kB]\nGet:7 http://archive.ubuntu.com/ubuntu focal/main amd64 libu2f-udev all 1.1.10-1 [6108 B]\nFetched 97.3 MB in 2s (40.1 MB/s)\nSelecting previously unselected package fonts-liberation.\n(Reading database ... 109489 files and directories currently installed.)\nPreparing to unpack .../fonts-liberation_1%3a1.07.4-11_all.deb ...\nUnpacking fonts-liberation (1:1.07.4-11) ...\nSelecting previously unselected package libwayland-server0:amd64.\nPreparing to unpack .../libwayland-server0_1.18.0-1ubuntu0.1_amd64.deb ...\nUnpacking libwayland-server0:amd64 (1.18.0-1ubuntu0.1) ...\nSelecting previously unselected package libgbm1:amd64.\nPreparing to unpack .../libgbm1_21.2.6-0ubuntu0.1~20.04.2_amd64.deb ...\nUnpacking libgbm1:amd64 (21.2.6-0ubuntu0.1~20.04.2) ...\nPreparing to unpack .../libudev1_245.4-4ubuntu3.21_amd64.deb ...\nUnpacking libudev1:amd64 (245.4-4ubuntu3.21) over (245.4-4ubuntu3.20) ...\nSetting up libudev1:amd64 (245.4-4ubuntu3.21) ...\nSelecting previously unselected package udev.\n(Reading database ... 109526 files and directories currently installed.)\nPreparing to unpack .../udev_245.4-4ubuntu3.21_amd64.deb ...\nUnpacking udev (245.4-4ubuntu3.21) ...\nSelecting previously unselected package libu2f-udev.\nPreparing to unpack .../libu2f-udev_1.1.10-1_all.deb ...\nUnpacking libu2f-udev (1.1.10-1) ...\nSelecting previously unselected package google-chrome-stable.\nPreparing to unpack .../google-chrome-stable_113.0.5672.63-1_amd64.deb ...\nUnpacking google-chrome-stable (113.0.5672.63-1) ...\nSetting up libwayland-server0:amd64 (1.18.0-1ubuntu0.1) ...\nSetting up libgbm1:amd64 (21.2.6-0ubuntu0.1~20.04.2) ...\nSetting up udev (245.4-4ubuntu3.21) ...\ninvoke-rc.d: could not determine current runlevel\ninvoke-rc.d: policy-rc.d denied execution of start.\nSetting up fonts-liberation (1:1.07.4-11) ...\nSetting up libu2f-udev (1.1.10-1) ...\nFailed to send reload request: No such file or directory\nSetting up google-chrome-stable (113.0.5672.63-1) ...\nupdate-alternatives: using /usr/bin/google-chrome-stable to provide /usr/bin/x-www-browser (x-www-browser) in auto mode\nupdate-alternatives: using /usr/bin/google-chrome-stable to provide /usr/bin/gnome-www-browser (gnome-www-browser) in auto mode\nupdate-alternatives: using /usr/bin/google-chrome-stable to provide /usr/bin/google-chrome (google-chrome) in auto mode\nProcessing triggers for libc-bin (2.31-0ubuntu9.9) ...\nProcessing triggers for systemd (245.4-4ubuntu3.20) ...\nProcessing triggers for man-db (2.9.1-1) ...\nProcessing triggers for fontconfig (2.13.1-2ubuntu3) ...\nProcessing triggers for mime-support (3.64ubuntu1) ...\n","output_type":"stream"}]},{"cell_type":"code","source":"# install chromedriver\n# !apt-get install -y qq unzip\n!wget -O /tmp/chromedriver.zip http://chromedriver.storage.googleapis.com/`curl -sS chromedriver.storage.googleapis.com/LATEST_RELEASE`/chromedriver_linux64.zip\n!unzip /tmp/chromedriver.zip chromedriver -d /usr/local/bin/","metadata":{"execution":{"iopub.status.busy":"2023-05-07T17:18:36.858509Z","iopub.execute_input":"2023-05-07T17:18:36.859109Z","iopub.status.idle":"2023-05-07T17:18:41.678656Z","shell.execute_reply.started":"2023-05-07T17:18:36.859072Z","shell.execute_reply":"2023-05-07T17:18:41.677302Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"curl: /opt/conda/lib/libcurl.so.4: no version information available (required by curl)\n--2023-05-07 17:18:38--  http://chromedriver.storage.googleapis.com/113.0.5672.63/chromedriver_linux64.zip\nResolving chromedriver.storage.googleapis.com (chromedriver.storage.googleapis.com)... 108.177.97.128, 2404:6800:4008:c00::80\nConnecting to chromedriver.storage.googleapis.com (chromedriver.storage.googleapis.com)|108.177.97.128|:80... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 7315061 (7.0M) [application/zip]\nSaving to: ‘/tmp/chromedriver.zip’\n\n/tmp/chromedriver.z 100%[===================>]   6.98M  6.85MB/s    in 1.0s    \n\n2023-05-07 17:18:40 (6.85 MB/s) - ‘/tmp/chromedriver.zip’ saved [7315061/7315061]\n\nArchive:  /tmp/chromedriver.zip\n  inflating: /usr/local/bin/chromedriver  \n","output_type":"stream"}]},{"cell_type":"code","source":"# install selenium\n!pip install -y python3-selenium\n# !pip install selenium==3.141.0 > /dev/null","metadata":{"execution":{"iopub.status.busy":"2023-05-07T17:18:41.681048Z","iopub.execute_input":"2023-05-07T17:18:41.681553Z","iopub.status.idle":"2023-05-07T17:18:43.651585Z","shell.execute_reply.started":"2023-05-07T17:18:41.681502Z","shell.execute_reply":"2023-05-07T17:18:43.650192Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"\nUsage:   \n  pip install [options] <requirement specifier> [package-index-options] ...\n  pip install [options] -r <requirements file> [package-index-options] ...\n  pip install [options] [-e] <vcs project url> ...\n  pip install [options] [-e] <local project path> ...\n  pip install [options] <archive url/path> ...\n\nno such option: -y\n","output_type":"stream"}]},{"cell_type":"code","source":"# To check Google Chrome's version\n!google-chrome --version","metadata":{"execution":{"iopub.status.busy":"2023-05-07T17:18:43.653422Z","iopub.execute_input":"2023-05-07T17:18:43.653815Z","iopub.status.idle":"2023-05-07T17:18:44.883778Z","shell.execute_reply.started":"2023-05-07T17:18:43.653778Z","shell.execute_reply":"2023-05-07T17:18:44.882559Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Google Chrome 113.0.5672.63 \n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install -U sentence-transformers \n!pip install selenium \n!pip install tqdm","metadata":{"execution":{"iopub.status.busy":"2023-05-07T17:18:44.885908Z","iopub.execute_input":"2023-05-07T17:18:44.886502Z","iopub.status.idle":"2023-05-07T17:19:24.298815Z","shell.execute_reply.started":"2023-05-07T17:18:44.886450Z","shell.execute_reply":"2023-05-07T17:19:24.297455Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Collecting sentence-transformers\n  Downloading sentence-transformers-2.2.2.tar.gz (85 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.0/86.0 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: transformers<5.0.0,>=4.6.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.28.1)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (4.64.1)\nRequirement already satisfied: torch>=1.6.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (2.0.0+cpu)\nRequirement already satisfied: torchvision in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.15.1+cpu)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.23.5)\nRequirement already satisfied: scikit-learn in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.2.2)\nRequirement already satisfied: scipy in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (1.9.3)\nRequirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (3.2.4)\nRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.1.98)\nRequirement already satisfied: huggingface-hub>=0.4.0 in /opt/conda/lib/python3.10/site-packages (from sentence-transformers) (0.13.4)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (2.28.2)\nRequirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (4.5.0)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.4.0->sentence-transformers) (3.11.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (1.11.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.6.0->sentence-transformers) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (2023.3.23)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers<5.0.0,>=4.6.0->sentence-transformers) (0.13.3)\nRequirement already satisfied: six in /opt/conda/lib/python3.10/site-packages (from nltk->sentence-transformers) (1.16.0)\nRequirement already satisfied: joblib>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (1.2.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from scikit-learn->sentence-transformers) (3.1.0)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /opt/conda/lib/python3.10/site-packages (from torchvision->sentence-transformers) (9.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.9->huggingface-hub>=0.4.0->sentence-transformers) (3.0.9)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.6.0->sentence-transformers) (2.1.2)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (1.26.15)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (3.4)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2.1.1)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.4.0->sentence-transformers) (2022.12.7)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.6.0->sentence-transformers) (1.3.0)\nBuilding wheels for collected packages: sentence-transformers\n  Building wheel for sentence-transformers (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for sentence-transformers: filename=sentence_transformers-2.2.2-py3-none-any.whl size=125938 sha256=48acbc965c9c2ae6cea79fc73d8ccc165c0f65cd03f2cf874f037487f8ec9eaf\n  Stored in directory: /root/.cache/pip/wheels/62/f2/10/1e606fd5f02395388f74e7462910fe851042f97238cbbd902f\nSuccessfully built sentence-transformers\nInstalling collected packages: sentence-transformers\nSuccessfully installed sentence-transformers-2.2.2\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mCollecting selenium\n  Downloading selenium-4.9.0-py3-none-any.whl (6.5 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.5/6.5 MB\u001b[0m \u001b[31m40.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hCollecting trio-websocket~=0.9\n  Downloading trio_websocket-0.10.2-py3-none-any.whl (17 kB)\nRequirement already satisfied: urllib3[socks]~=1.26 in /opt/conda/lib/python3.10/site-packages (from selenium) (1.26.15)\nRequirement already satisfied: certifi>=2021.10.8 in /opt/conda/lib/python3.10/site-packages (from selenium) (2022.12.7)\nCollecting trio~=0.17\n  Downloading trio-0.22.0-py3-none-any.whl (384 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m384.9/384.9 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting async-generator>=1.9\n  Downloading async_generator-1.10-py3-none-any.whl (18 kB)\nCollecting exceptiongroup>=1.0.0rc9\n  Downloading exceptiongroup-1.1.1-py3-none-any.whl (14 kB)\nRequirement already satisfied: sortedcontainers in /opt/conda/lib/python3.10/site-packages (from trio~=0.17->selenium) (2.4.0)\nRequirement already satisfied: sniffio in /opt/conda/lib/python3.10/site-packages (from trio~=0.17->selenium) (1.3.0)\nRequirement already satisfied: attrs>=19.2.0 in /opt/conda/lib/python3.10/site-packages (from trio~=0.17->selenium) (22.2.0)\nCollecting outcome\n  Downloading outcome-1.2.0-py2.py3-none-any.whl (9.7 kB)\nRequirement already satisfied: idna in /opt/conda/lib/python3.10/site-packages (from trio~=0.17->selenium) (3.4)\nCollecting wsproto>=0.14\n  Downloading wsproto-1.2.0-py3-none-any.whl (24 kB)\nRequirement already satisfied: PySocks!=1.5.7,<2.0,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from urllib3[socks]~=1.26->selenium) (1.7.1)\nRequirement already satisfied: h11<1,>=0.9.0 in /opt/conda/lib/python3.10/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.14.0)\nInstalling collected packages: wsproto, outcome, exceptiongroup, async-generator, trio, trio-websocket, selenium\nSuccessfully installed async-generator-1.10 exceptiongroup-1.1.1 outcome-1.2.0 selenium-4.9.0 trio-0.22.0 trio-websocket-0.10.2 wsproto-1.2.0\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (4.64.1)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"markdown","source":"# **Import packages, Dataset, Pre-trained Models and Pre-computed Embeddings**\n","metadata":{}},{"cell_type":"code","source":"import torch\nfrom sentence_transformers import SentenceTransformer\n\nif torch.cuda.is_available():\n    device = torch.device('cuda')\nelse:\n    device = torch.device('cpu')\n\nmodel_path = \"./archive/sbert-models/paraphrase-multilingual-mpnet-base-v2\"\n\nmodel = SentenceTransformer(model_path, device=device)\n# embedding = model.encode(text, device=device)","metadata":{"execution":{"iopub.status.busy":"2023-05-07T17:19:42.698459Z","iopub.execute_input":"2023-05-07T17:19:42.698889Z","iopub.status.idle":"2023-05-07T17:19:47.650053Z","shell.execute_reply.started":"2023-05-07T17:19:42.698850Z","shell.execute_reply":"2023-05-07T17:19:47.648772Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.5\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[6], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m     device \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mdevice(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m model_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./archive/sbert-models/paraphrase-multilingual-mpnet-base-v2\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m---> 11\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mSentenceTransformer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# embedding = model.encode(text, device=device)\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/sentence_transformers/SentenceTransformer.py:77\u001b[0m, in \u001b[0;36mSentenceTransformer.__init__\u001b[0;34m(self, model_name_or_path, modules, device, cache_folder, use_auth_token)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;66;03m#Not a path, load from hub\u001b[39;00m\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01min\u001b[39;00m model_name_or_path \u001b[38;5;129;01mor\u001b[39;00m model_name_or_path\u001b[38;5;241m.\u001b[39mcount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 77\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPath \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m not found\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(model_name_or_path))\n\u001b[1;32m     79\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m model_name_or_path \u001b[38;5;129;01mand\u001b[39;00m model_name_or_path\u001b[38;5;241m.\u001b[39mlower() \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m basic_transformer_models:\n\u001b[1;32m     80\u001b[0m         \u001b[38;5;66;03m# A model from sentence-transformers\u001b[39;00m\n\u001b[1;32m     81\u001b[0m         model_name_or_path \u001b[38;5;241m=\u001b[39m __MODEL_HUB_ORGANIZATION__ \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m model_name_or_path\n","\u001b[0;31mValueError\u001b[0m: Path ./archive/sbert-models/paraphrase-multilingual-mpnet-base-v2 not found"],"ename":"ValueError","evalue":"Path ./archive/sbert-models/paraphrase-multilingual-mpnet-base-v2 not found","output_type":"error"}]},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport re\nimport string\nimport nltk\nimport gensim\nfrom gensim.models import Word2Vec\n# from nltk.corpus import stopwords\n# from nltk.tokenize import word_tokenize\nimport sentence_transformers\nfrom scipy import spatial\nfrom tqdm.notebook import tqdm\ntqdm.pandas()\n# \n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\n\nimport requests\nfrom bs4 import BeautifulSoup\nimport csv\nimport pandas as pd\nimport re\nfrom selenium import webdriver\nfrom selenium.webdriver.common.by import By\nfrom selenium.webdriver.common.keys import Keys\nfrom selenium.webdriver.support.ui import WebDriverWait\nfrom selenium.webdriver.support import expected_conditions as EC\nimport time\n\noptions = webdriver.ChromeOptions()\noptions.add_argument('--headless')\noptions.add_argument(\"--no-sandbox\")\noptions.add_argument(\"--headless\")\noptions.add_argument(\"--disable-gpu\")\n# driver = webdriver.Chrome(PATH)","metadata":{"execution":{"iopub.status.busy":"2023-05-07T17:19:47.650938Z","iopub.status.idle":"2023-05-07T17:19:47.651844Z","shell.execute_reply.started":"2023-05-07T17:19:47.651652Z","shell.execute_reply":"2023-05-07T17:19:47.651671Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_dataframe = pd.read_csv(\"./archive/books1.csv\")\nembedding1 = np.load('./archive/embeddings.npy', allow_pickle = True)\nembedding2 = np.load('./archive/embeddings (1).npy', allow_pickle = True)\nembedding4 = np.load('./archive/embeddings2 (1).npy', allow_pickle = True)\nembedding5 = np.load('./archive/embeddings2.npy', allow_pickle = True)\n\nembedding_Description = np.load('./archive/D-embeddings.npy', allow_pickle=True)\nembedding_beautified = np.load('./archive/D-embeddings (1).npy', allow_pickle=True)\n\npd1 = pd.DataFrame(embedding1)\npd2 = pd.DataFrame(embedding2)\npd4 = pd.DataFrame(embedding4)\npd5 = pd.DataFrame(embedding5)\n\nfinal = pd.concat([pd1, pd2,pd4, pd5])\nfinal.rename(columns={0:\"embeddings\"}, inplace= True)\n# input_dataframe.head()\nfinal.index = pd.RangeIndex(start=0, stop = 0+len(final), step = 1)\nfinal = final.reset_index()\nfinalAllDescriptionEmbeddings = final\n\nfin_emb = []\nfor i in range(len(embedding_Description)):\n    fin_emb.append(embedding_Description[i].tolist())\nfin_emb\n\nfin_emb1 = []\nfor i in range(len(embedding_Description)):\n    fin_emb1.append(embedding_Description[i].tolist())\nfin_emb1\n\nfinalDescriptionEmbeddings = pd.DataFrame({'embeddings':fin_emb})\nfinalBeautifiedEmbeddings = pd.DataFrame({'embeddings':fin_emb1})\nfinalBeautifiedEmbeddings = finalBeautifiedEmbeddings.reset_index()\nfinalDescriptionEmbeddings = finalDescriptionEmbeddings.reset_index()\n# finalDescriptionEmbeddings","metadata":{"execution":{"iopub.status.busy":"2023-05-07T17:19:47.652630Z","iopub.status.idle":"2023-05-07T17:19:47.652967Z","shell.execute_reply.started":"2023-05-07T17:19:47.652804Z","shell.execute_reply":"2023-05-07T17:19:47.652820Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Let's get started!**","metadata":{}},{"cell_type":"code","source":"def extractUserBookData(likedBooks_title):\n    global newUser, lengthy, genre, likedBooks_data\n    # Set up the Selenium driver\n    driver = webdriver.Chrome(options=options)\n    for book_name in likedBooks_title:\n        print(\"Getting data for the book: \",book_name)\n        driver.get(\"https://www.goodreads.com/\")\n        WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, \"//input[@id='sitesearch_field']\"))).send_keys(book_name + Keys.RETURN)\n        driver.implicitly_wait(10)  \n        element = driver.find_element(By.XPATH, \"//a[@class='bookTitle']\")\n        driver.execute_script(\"arguments[0].click();\", element)\n\n        # WebDriverWait(driver, 20).until(EC.element_to_be_clickable((By.XPATH, \"//a[@class='bookTitle']\"))).click()\n        cur_url = str(driver.current_url)\n        # print(cur_url)\n        url = cur_url.split('?')[0]\n        # print(url)\n        # driver.get(url)\n\n        driver.implicitly_wait(10)  \n        element = driver.find_element(By.XPATH, '//span[contains(text(),\"Book details & editions\")]')\n        driver.execute_script(\"arguments[0].click();\", element)\n        element = driver.find_element(By.XPATH, '//span[contains(text(),\"Show more\")]')\n        driver.execute_script(\"arguments[0].click();\", element)\n        element = driver.find_element(By.XPATH, '//span[contains(text(),\"Show more\")]')\n        driver.execute_script(\"arguments[0].click();\", element)\n        element = driver.find_element(By.XPATH, '//span[contains(text(),\"...more\")]')\n        driver.execute_script(\"arguments[0].click();\", element)\n\n        # Get the html page content from the driver\n        content = driver.page_source\n\n        # Parse the HTML content using BeautifulSoup\n        book_soup = BeautifulSoup(content, 'html.parser')\n        # print(soup.prettify())\n        #################################### Extract the book details#######################################\n        # Extract book id \n        match = re.search(r\"\\d+\", cur_url)\n        if match:\n            book_id = match.group()\n        else: book_id=\"\"\n        # Extract book title\n        book_title=book_soup.find('h1', {'data-testid': 'bookTitle'}).text.strip()\n        # print(book_title)\n\n        # Extract book description\n        book_description = book_soup.find_all('div', class_='DetailsLayoutRightParagraph__widthConstrained')[0].text.strip()\n        # print(book_description)\n\n        # Extract book generes\n        genre = book_soup.find_all('span', class_='BookPageMetadataSection__genreButton')\n        genres=[]\n        for i in range(len(genre)):\n            genres.append(genre[i].text.strip())\n        # print(genres)\n\n        # Extract book author\n        book_author = book_soup.find_all('span', class_='ContributorLink__name')[0].text.strip() \n        # print(book_author)\n\n        #Extract num of ratings\n        num_ratings=book_soup.find('span', {'data-testid':\"ratingsCount\"}).text.strip()[:-8].replace(\",\", \"\")\n        # print(num_ratings)\n\n        # Extract number of book pages\n        num_pages = book_soup.find('p', {'data-testid':\"pagesFormat\"}).text.strip().split(\",\")[0]\n        # print(num_pages)\n\n        # Extract first published date\n        first_published = book_soup.find('p', {'data-testid':\"publicationInfo\"}).text.strip()[16:]\n        # print(first_published)\n\n        # Extract book awards\n        try:\n            awards = book_soup.find_all(\"span\", {\"data-testid\": \"award\"})\n            book_awards=[]\n            for i in range(len(awards)):\n                book_awards.append(awards[i].text.strip())\n        except AttributeError:\n            book_awards=[]\n        # print(book_awards)\n\n        # Extract book series\n        try:\n            book_series = book_soup.find('h3', {'class':'Text Text__title3 Text__italic Text__regular Text__subdued'}).text.strip()\n        except AttributeError:\n            book_series=\"\"\n        # print(book_series)\n\n        # Extract book avg rating\n        avg_rating = book_soup.find('div', {'class':'RatingStatistics__rating'}).text.strip()\n        # print(avg_rating)\n\n        # Extract book individuak ratings\n        ratingsByStars=[]\n        ratingsByStars.append(book_soup.find('div', {'data-testid':\"labelTotal-5\",'class':'RatingsHistogram__labelTotal'}).text.strip().split(\" \")[0].replace(\",\",\"\"))\n        ratingsByStars.append(book_soup.find('div', {'data-testid':\"labelTotal-4\",'class':'RatingsHistogram__labelTotal'}).text.strip().split(\" \")[0].replace(\",\",\"\"))\n        ratingsByStars.append(book_soup.find('div', {'data-testid':\"labelTotal-3\",'class':'RatingsHistogram__labelTotal'}).text.strip().split(\" \")[0].replace(\",\",\"\"))\n        ratingsByStars.append(book_soup.find('div', {'data-testid':\"labelTotal-2\",'class':'RatingsHistogram__labelTotal'}).text.strip().split(\" \")[0].replace(\",\",\"\"))\n        ratingsByStars.append(book_soup.find('div', {'data-testid':\"labelTotal-1\",'class':'RatingsHistogram__labelTotal'}).text.strip().split(\" \")[0].replace(\",\",\"\"))\n        # print(ratingsByStars)\n        #Append the book data to the list\n        likedBooks_data.append([book_id, book_title,book_series,book_author, avg_rating, book_description, genres, num_pages, first_published, book_awards, num_ratings, ratingsByStars])\n    # Create a dataframe from the list of book data and save it to a csv file\n    books_df = pd.DataFrame(likedBooks_data, columns=['id', 'title','series','author', 'ratings', 'description', 'genres', 'num_pages', 'first_published', 'awards', 'num_ratings', 'ratingsByStars'])\n    books_df.to_csv('./archive/UserBooks.csv', index=False, encoding='utf-8')\n    driver.close()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **RECOMMENDATION MODELS**","metadata":{}},{"cell_type":"markdown","source":"# **PREPROCESS DATA FOR RECOMMENDATION**","metadata":{}},{"cell_type":"code","source":"input_dataframe = input_dataframe.loc[:, [\"title\",\"description\", \"author\", \"genres\",\"likedPercent\"]]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"books_df = pd.read_csv(\"./archive/UserBooks.csv\")\nbooks_df = books_df.loc[:, [\"title\", \"description\", \"author\", \"genres\", \"ratings\"]]\n\ndef convertPercent(text):\n    temp = float(text)\n    likedPercent = temp/5*100\n    text = likedPercent\n    return text\nbooks_df.ratings = books_df.ratings.apply(convertPercent)\nbooks_df.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"input_dataframe.dropna(subset=[\"description\"], inplace=True)\n\nurl_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\ndef remove_url(text):   \n    return re.sub(url_pattern, r'', text)\n\nhtml_pattern = re.compile('<[^>]*>')\ndef clean_html_tags(text):\n    return re.sub(html_pattern, r'', text)\n\npunctuations = string.punctuation\ndef remove_punctuations(text):\n    return text.translate(str.maketrans('', '', punctuations))\n\ndef text_lowercase(text):\n    return text.lower()\n\ndef preprocess(input_dataframe):\n    input_dataframe.description = input_dataframe.description.apply(remove_url)\n    input_dataframe.description = input_dataframe.description.apply(clean_html_tags)\n    input_dataframe.description = input_dataframe.description.apply(remove_punctuations)\n    input_dataframe.description = input_dataframe.description.apply(text_lowercase)\n    input_dataframe.dropna(subset=[\"description\"], inplace=True)\n    return input_dataframe\n\ninput_dataframe = preprocess(input_dataframe)\nbooks_df = preprocess(books_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **FORMAT DATAFRAME FOR SPECIFIC RECOMMENDATION**","metadata":{}},{"cell_type":"code","source":"liked_book_descriptionBeautified = []\nfor i in range(len(books_df)):\n    sentence = \" \"\n    sentence += \"Description of the book is - \" + str(books_df.iloc[i][\"description\"])\n    sentence += \". \"\n    sentence += \"The author of the book is - \" + books_df.iloc[i][\"author\"]\n    sentence += \". \"\n    sentence += \"The genres of the book are - \"\n    sentence +=  str(books_df.iloc[i][\"genres\"][1:-2])\n    sentence += \". \"\n    sentence += \"The rating of the book is - \" + str(books_df.iloc[i][\"ratings\"])\n    liked_book_descriptionBeautified.append(sentence)\nliked_book_descriptionBeauty = pd.DataFrame()\nliked_book_descriptionBeauty['description'] = liked_book_descriptionBeautified\n# liked_book_descriptionBeautified = books_df['description'] + books_df['author'] + str(books_df['genres']) + str(books_df['ratings'])\nliked_book_descriptionAll =books_df['description'] + books_df['author'] + str(books_df['genres']) \nliked_book_description =books_df['description']\nliked_book_descriptionBeauty.iloc[0][\"description\"]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"textBeautified = []\nfor i in range(len(input_dataframe)):\n    sentence = \" \"\n    sentence += \"Description of the book is - \" + str(input_dataframe.iloc[i][\"description\"])\n    sentence += \". \"\n    sentence += \"The author of the book is - \" + input_dataframe.iloc[i][\"author\"]\n    sentence += \". \"\n    sentence += \"The genres of the book are - \"\n    sentence +=  input_dataframe.iloc[i][\"genres\"][1:-2]\n    sentence += \". \"\n    sentence += \"The rating of the book is - \" + str(input_dataframe.iloc[i][\"likedPercent\"])\n    textBeautified.append(sentence)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"textAll = input_dataframe[['description', 'author', 'genres']].agg(' '.join, axis=1)\ndataAll = textAll\ntext = input_dataframe[\"description\"]\ndata = text\ndata.iloc[1]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **WORD2VEC Model for recommendation**","metadata":{}},{"cell_type":"code","source":"# test = data\n# test_df = test.to_frame()\n# test_df.columns = ['concatenated']\n# print(test_df.columns)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def get_w2v(input):\n# #     list_sentences = input.apply(lambda x: ' '.join(x.dropna().astype(str)), axis=1).tolist()\n#     list_sentences = input['concatenated'].astype(str).tolist()\n#     sentences = [sentence for text in list_sentences for sentence in nltk.sent_tokenize(text)]\n#     stopwords = nltk.corpus.stopwords.words('english')\n#     preprocessed_sentences = [[word.lower() for word in nltk.word_tokenize(sentence) if word.lower() not in stopwords] for sentence in sentences]\n#     CBOW = gensim.models.Word2Vec(preprocessed_sentences, min_count = 1, vector_size = 100, window = 5, sg = 0)\n#     SGRAM = gensim.models.Word2Vec(preprocessed_sentences, min_count = 1, vector_size = 100, window = 5, sg = 1)\n#     return CBOW, SGRAM, preprocessed_sentences\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# CBOW, SGRAM, preprocessed_sentences = get_w2v(test_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def getCBOWEmbeddingList(preprocessed_sentences):\n#     w2v_embeddings = []\n#     for i in range(len(preprocessed_sentences)):\n#         for j in range(len(preprocessed_sentences[i])):\n#             if j == 0:\n#                 test = np.zeros(100)\n#                 test += CBOW.wv[preprocessed_sentences[i][j]]\n#             else:\n#                 test += CBOW.wv[preprocessed_sentences[i][j]]\n#         test/=len(preprocessed_sentences[i])\n#         w2v_embeddings.append(test)\n#     return w2v_embeddings\n# w2v_embeddings = getCBOWEmbeddingList(preprocessed_sentences)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def findRecommendationW2V(w2v_embeddings):\n#     test = w2v_embeddings[0]\n#     test = np.asarray(test)\n#     similarity = []\n#     for i in w2v_embeddings:\n#         i = np.asarray(i)\n#         similarity.append(-1*(spatial.distance.cosine(i, test)-1))\n#     sim = pd.DataFrame(similarity)\n#     sim.columns = ['cosine_sim']\n#     sim = sim.reset_index()\n#     result = sim.sort_values(by=['cosine_sim'], ascending = False)\n    \n#     recommend_books = []\n#     recommend_books.append(input_dataframe.iloc[int(result.iloc[1]['index'])])\n#     recommend_books.append(input_dataframe.iloc[int(result.iloc[2]['index'])])\n#     recommend_books = pd.DataFrame(recommend_books)\n#     return recommend_books\n\n# recommended_books = findRecommendationW2V(w2v_embeddings)\n# recommended_books.anyhead()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# def getSGRAMEmbeddingList(preprocessed_sentences):\n#     w2v_embeddings = []\n#     for i in range(len(preprocessed_sentences)):\n#         for j in range(len(preprocessed_sentences[i])):\n#             if j == 0:\n#                 test = np.zeros(100)\n#                 test += SGRAM.wv[preprocessed_sentences[i][j]]\n#             else:\n#                 test += SGRAM.wv[preprocessed_sentences[i][j]]\n#         test/=len(preprocessed_sentences[i])\n#         w2v_embeddings.append(test)\n#     return w2v_embeddings\n# w2v_embeddings = getSGRAMEmbeddingList(preprocessed_sentences)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# recommended_books = findRecommendationW2V(w2v_embeddings)\n# recommended_books.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **FIND EMBEDDINGS**","metadata":{}},{"cell_type":"code","source":"# data = np.array(data).tolist()\n# data[22404]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# data = textBeautified\n# def get_embeddings(text):\n#     return model.encode(text, device = device)\n# embeddings = get_embeddings(data)\n# np.save('D-embeddings', embeddings)\n# # data = data.iloc[:15000]\n# # temp1 = data.progress_apply(get_embeddings)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **TRANSFORMER BASED RECOMMENDATION**\nRecommends similar books for each book individually.\nRecommends based context of both books read'","metadata":{}},{"cell_type":"code","source":"def getSimilarityDescriptionScores(description = text.iloc[1733]):\n    test_embedding = model.encode(description, device = device)\n    similarity = []\n    for i in finalDescriptionEmbeddings['embeddings']:\n        similarity.append(-1*(spatial.distance.cosine(i, test_embedding)-1))\n    finalDescriptionEmbeddings['cosine_sim'] = similarity\n    result = finalDescriptionEmbeddings.sort_values(by=['cosine_sim'], ascending = False)\n    return result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getSimilarityScores(description = text.iloc[1733]):\n    test_embedding = model.encode(description, device = device)\n    similarity = []\n    for i in finalAllDescriptionEmbeddings['embeddings']:\n        similarity.append(-1*(spatial.distance.cosine(i, test_embedding)-1))\n    finalAllDescriptionEmbeddings['cosine_sim'] = similarity\n    result = finalAllDescriptionEmbeddings.sort_values(by=['cosine_sim'], ascending = False)\n    return result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def becauseYouLiked(liked_books_description):\n    recommend_books = []\n    for liked_book in liked_books_description:\n        fin = getSimilarityScores(liked_book)\n        recommend_books.append(input_dataframe.iloc[int(fin.iloc[1]['index'])])\n        recommend_books.append(input_dataframe.iloc[int(fin.iloc[2]['index'])])\n    recommend_books = pd.DataFrame(recommend_books)\n    return recommend_books\nrecommended_books = becauseYouLiked(liked_book_descriptionAll)\nrecommended_books","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def becauseYouLikedDescription(liked_books_description):\n    recommend_books = []\n    for liked_book in liked_books_description:\n        fin = getSimilarityDescriptionScores(liked_book)\n        recommend_books.append(input_dataframe.iloc[int(fin.iloc[1]['index'])])\n        recommend_books.append(input_dataframe.iloc[int(fin.iloc[2]['index'])])\n    recommend_books = pd.DataFrame(recommend_books)\n    return recommend_books\nrecommended_books = becauseYouLikedDescription(liked_book_description)\nrecommended_books","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def youMayAlsoLike(liked_book_description):\n    combined_input = \"\"\n    for liked_book in liked_book_description:\n        combined_input += liked_book\n        combined_input += \"\\n\"\n    similarity = []\n    test_embedding = model.encode(combined_input, device = device)\n    for book_embeddings in finalDescriptionEmbeddings['embeddings']:\n        similarity.append(-1*(spatial.distance.cosine(book_embeddings, test_embedding)-1))\n    finalDescriptionEmbeddings['cosine_sim'] = similarity\n    result = finalDescriptionEmbeddings.sort_values(by=['cosine_sim'], ascending = False)\n    contextBasedPrediction = []\n    contextBasedPrediction.append(input_dataframe.iloc[result.iloc[1][0]])\n    contextBasedPrediction.append(input_dataframe.iloc[result.iloc[2][0]])\n    contextBasedPrediction.append(input_dataframe.iloc[result.iloc[3][0]])\n    contextBasedPrediction.append(input_dataframe.iloc[result.iloc[4][0]])\n    contextBasedPrediction.append(input_dataframe.iloc[result.iloc[5][0]])\n    contextBasedPrediction = pd.DataFrame(contextBasedPrediction)\n    return contextBasedPrediction\n\ncontextBasedPrediction = youMayAlsoLike(liked_book_description)\ncontextBasedPrediction","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def youMayAlsoLikeAveraged(liked_book_description):\n    combined_input = \"\"\n    count = 0\n    for liked_book in liked_book_description:\n        if count == 0:\n            test_embedding = np.zeros(768)\n            test_embedding += model.encode(liked_book, device = device)\n        count+=1\n    test_embedding /= count\n    similarity = []\n    for book_embeddings in finalDescriptionEmbeddings['embeddings']:\n        similarity.append(-1*(spatial.distance.cosine(book_embeddings, test_embedding)-1))\n    finalDescriptionEmbeddings['cosine_sim'] = similarity\n    result = finalDescriptionEmbeddings.sort_values(by=['cosine_sim'], ascending = False)\n    contextBasedPrediction = []\n    contextBasedPrediction.append(input_dataframe.iloc[result.iloc[1][0]])\n    contextBasedPrediction.append(input_dataframe.iloc[result.iloc[2][0]])\n    contextBasedPrediction.append(input_dataframe.iloc[result.iloc[3][0]])\n    contextBasedPrediction.append(input_dataframe.iloc[result.iloc[4][0]])\n    contextBasedPrediction.append(input_dataframe.iloc[result.iloc[5][0]])\n    contextBasedPrediction = pd.DataFrame(contextBasedPrediction)\n    return contextBasedPrediction\n\ncontextBasedPrediction = youMayAlsoLikeAveraged(liked_book_description)\ncontextBasedPrediction","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getSimilarityDescriptionBScores(description = text.iloc[1733]):\n    test_embedding = model.encode(description, device = device)\n    similarity = []\n    for i in finalBeautifiedEmbeddings['embeddings']:\n        similarity.append(-1*(spatial.distance.cosine(i, test_embedding)-1))\n    finalBeautifiedEmbeddings['cosine_sim'] = similarity\n    result = finalBeautifiedEmbeddings.sort_values(by=['cosine_sim'], ascending = False)\n    return result\n\ndef becauseYouLikedBeautifiedDescription(liked_books_description):\n    recommend_books = []\n    for liked_book in liked_books_description['description']:\n        # print(liked_book)\n        fin = getSimilarityDescriptionBScores(liked_book)\n        recommend_books.append(input_dataframe.iloc[int(fin.iloc[1]['index'])])\n        recommend_books.append(input_dataframe.iloc[int(fin.iloc[2]['index'])])\n    recommend_books = pd.DataFrame(recommend_books)\n    return recommend_books\nrecommended_books = becauseYouLikedBeautifiedDescription(liked_book_descriptionBeauty)\nrecommended_books","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def youMayAlsoLikeAveragedB(liked_book_description):\n    combined_input = \"\"\n    count = 0\n    for liked_book in liked_book_description:\n        if count == 0:\n            test_embedding = np.zeros(768)\n            test_embedding += model.encode(liked_book, device = device)\n        count+=1\n    test_embedding /= count\n    similarity = []\n    for book_embeddings in finalBeautifiedEmbeddings['embeddings']:\n        similarity.append(-1*(spatial.distance.cosine(book_embeddings, test_embedding)-1))\n    finalBeautifiedEmbeddings['cosine_sim'] = similarity\n    result = finalBeautifiedEmbeddings.sort_values(by=['cosine_sim'], ascending = False)\n    contextBasedPrediction = []\n    contextBasedPrediction.append(input_dataframe.iloc[result.iloc[1][0]])\n    contextBasedPrediction.append(input_dataframe.iloc[result.iloc[2][0]])\n    contextBasedPrediction.append(input_dataframe.iloc[result.iloc[3][0]])\n    contextBasedPrediction.append(input_dataframe.iloc[result.iloc[4][0]])\n    contextBasedPrediction.append(input_dataframe.iloc[result.iloc[5][0]])\n    contextBasedPrediction = pd.DataFrame(contextBasedPrediction)\n    return contextBasedPrediction\n\ncontextBasedPrediction = youMayAlsoLikeAveragedB(liked_book_description)\ncontextBasedPrediction","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def youMayAlsoLikeB(liked_book_description):\n    combined_input = \"\"\n    for liked_book in liked_book_description:\n        combined_input += liked_book\n        combined_input += \"\\n\"\n    similarity = []\n    test_embedding = model.encode(combined_input, device = device)\n    for book_embeddings in finalBeautifiedEmbeddings['embeddings']:\n        similarity.append(-1*(spatial.distance.cosine(book_embeddings, test_embedding)-1))\n    finalBeautifiedEmbeddings['cosine_sim'] = similarity\n    result =finalBeautifiedEmbeddings.sort_values(by=['cosine_sim'], ascending = False)\n    contextBasedPrediction = []\n    contextBasedPrediction.append(input_dataframe.iloc[result.iloc[1][0]])\n    contextBasedPrediction.append(input_dataframe.iloc[result.iloc[2][0]])\n    contextBasedPrediction.append(input_dataframe.iloc[result.iloc[3][0]])\n    contextBasedPrediction.append(input_dataframe.iloc[result.iloc[4][0]])\n    contextBasedPrediction.append(input_dataframe.iloc[result.iloc[5][0]])\n    contextBasedPrediction = pd.DataFrame(contextBasedPrediction)\n    return contextBasedPrediction\n\ncontextBasedPrediction = youMayAlsoLikeB(liked_book_description)\ncontextBasedPrediction","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Other functions","metadata":{}},{"cell_type":"code","source":"temp_df = pd.read_csv(\"./archive/books1.csv\")\n# Compute the top5 books for each genre\ngenreDict = dict()          ## Stores the top 5 book title per genre\ndef ComputeTopBooks_allGenre():\n    ## Finding the top 5 books for each genre available\n    temp_df['genres']=temp_df['genres'].apply(eval)\n    ratingList = dict()         ## Top 5 rating per genre\n    lowest = 5\n    for row in range(len(temp_df)):\n        genre = temp_df.loc[row, \"genres\"]\n        for g in genre:\n            if g in genreDict:\n                if len(genreDict[g]) < 5:\n                    genreDict[g].append(temp_df[\"title\"][row])\n                    ratingList[g].append(temp_df[\"rating\"][row])\n                    sorted(ratingList[g], reverse=True)\n                else:\n                    if temp_df[\"rating\"][row] > ratingList[g][4] and temp_df[\"numRatings\"][row] > 1000:\n                        genreDict[g][4] = temp_df[\"title\"][row]\n                        ratingList[g][4] = temp_df[\"rating\"][row]\n                        sorted(ratingList[g], reverse=True)\n            else:\n                genreDict[g] = []\n                ratingList[g] = []\n                genreDict[g].append(temp_df[\"title\"][row])\n                ratingList[g].append(temp_df[\"rating\"][row])\n\n# ComputeTopBooks_allGenre() # compute the once and for all the top 5 books of each genre","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getRecommedation_NewReader():\n    #recomending the top 5 books for each genre the user likes\n    for g in genre:\n        book_list=genreDict[g]\n        print(\"You may also like these books in \", genre,\" :\")\n        for book_title in book_list:\n            print(\"- \" + book_title)\n        print()    \n        ","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getRecommedation_ExpReader():\n    # print(likedBooks_title)\n    extractUserBookData(likedBooks_title)\n#     print(\"Reccomending for best books which are similar to each books(individually) in the list: \")\n#     print(\"\\n\\n\")\n#     print(\"Reccomending similar books wrt genres,author,description and ratings\")\n#     result_df=becauseYouLikedBeautifiedDescription(books_df)\n#     # print(result_df)\n#     print(\"\\n\\n\")\n#     print(\"Reccomending similar books wrt only description\")\n#     result_df=becauseYouLikedDescription(books_df)\n#     # print(result_df)\n#     print(\"Reccomending similar books wrt genres,author,description\")\n#     result_df=becauseYouLiked(books_df)\n#     # print(result_df)\n#     print(\"\\n\\n\")\n\n#     print(\"**************************************************************************************************************\")\n#     print(\"**************************************************************************************************************\")\n#     print(\"**************************************************************************************************************\")\n#     print(\"Reccomending for best books which are similar to all books(combined) in the list\")\n    \n    print(\"Reccomending similar books wrt genres,author,description,ratings\")\n    print(\"Average of all the embeddings of the books in the userlist is taken and then cosine similarity is calculated with all the books in the dataset\")\n    result_df= youMayAlsoLikeAveragedB(books_df)\n    # print(result_df)\n    print(\"\\n\\n\")\n\n    print(\"Concatinate all the embeddings of the books in the userlist and then cosine similarity is calculated with all the books in the dataset\")\n    result_df= youMayAlsoLikeB(books_df)\n    # print(result_df)\n    print(\"\\n\\n\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# **Start Recommendation Engine by taking input from User**","metadata":{}},{"cell_type":"code","source":"def StartReccSystem():\n    global newUser, lengthy, genre, likedBooks_title, likedBooks_data\n    print(\"Welcome to Book Recommender System\\n\")\n    \n    userinput = input(\"Do you have a prior reading experience (Y/n)?\")\n    if userinput.lower() == 'y':\n        newUser = True\n        likedBooks_title = input(\"Enter the title of books that you liked the most (separated by semicolon)\").split(sep=';')\n    elif userinput.lower() == 'n':\n        newUser = False\n        genre = input(\"Enter the genres you are interested in (separated by semicolon): \").split(sep=';')\n    \n    if (input(\"Are you interested in reading lenghty books (Y/n)?\").lower() == 'y'):\n        lengthy = True\n    else:\n        lengthy = False\n        \n    if newUser==True:\n        getRecommedation_ExpReader()\n    else:\n        getRecommedation_NewReader()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"StartReccSystem()\nnewUser = True\nlengthy = False\ngenre = []\nlikedBooks_title = []\nlikedBooks_data=[]","metadata":{},"execution_count":108,"outputs":[{"name":"stdout","output_type":"stream","text":"Welcome to Book Recommender System\n\n\n"}]},{"cell_type":"code","source":"newUser = True\nlengthy = False\ngenre = []\nlikedBooks_title = []\nlikedBooks_data=[]","metadata":{},"execution_count":null,"outputs":[]}]}